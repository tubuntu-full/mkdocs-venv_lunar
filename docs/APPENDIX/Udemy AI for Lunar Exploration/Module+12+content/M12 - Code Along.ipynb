{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":489236,"sourceType":"datasetVersion","datasetId":202982}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Building and Training a UNet Model on Moon Data\n","---"],"metadata":{"id":"xksEYwr8QXgN"}},{"cell_type":"markdown","source":["### About the Dataset\n","\n","This dataset contains 9766 realistic renders of lunar landscapes and their masks (segmented into three classes: sky, small rocks, bigger rocks). Additionally, a csv file of bounding boxes and cleaned masks of ground truths are provided.\n","\n","An interesting feature of this dataset is that the images are synthetic; they were created using Planetside Software's Terragen. This isn't too obvious immediately as the renderings are highly realistic but it does make more sense after taking into account the scarcity of space imagery data.\n","\n","Acknowledgment: Romain Pessia and Genya Ishigami of the Space Robotics Group, Keio University, Japan. You can find the datasetÂ https://www.kaggle.com/romainpessia/artificial-lunar-rocky-landscape-dataset"],"metadata":{"id":"jcPmSyZzQXgP"}},{"cell_type":"markdown","source":["### Reminder to turn on your GPU accelerator, from right hand side of your kaggle notebook, under Settings."],"metadata":{"id":"9XMsRC-aTdIi"}},{"cell_type":"markdown","source":["### Importing libraries\n","\n"],"metadata":{"id":"vneA5pJxQXgR"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import keras"],"metadata":{"id":"bGIcq3_DQXgT","execution":{"iopub.status.busy":"2024-01-06T10:52:12.784782Z","iopub.execute_input":"2024-01-06T10:52:12.785475Z","iopub.status.idle":"2024-01-06T10:52:25.806160Z","shell.execute_reply.started":"2024-01-06T10:52:12.785434Z","shell.execute_reply":"2024-01-06T10:52:25.805392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preprocessing"],"metadata":{"id":"OdS9NTtwQXgV"}},{"cell_type":"code","source":["# Save the render and clean paths for img_dir and mask_dir respectively\n","img_dir =\n","mask_dir =\n","\n","\n","# Create lists of images and masks present in the respective directories\n","images =\n","masks ="],"metadata":{"id":"SJ1vD73xTdIp","execution":{"iopub.status.busy":"2024-01-06T10:53:00.422263Z","iopub.execute_input":"2024-01-06T10:53:00.423210Z","iopub.status.idle":"2024-01-06T10:53:01.010176Z","shell.execute_reply.started":"2024-01-06T10:53:00.423165Z","shell.execute_reply":"2024-01-06T10:53:01.009258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if the output is as expected before moving further\n","\n","## Check first 5 elements of images and masks lists\n"],"metadata":{"id":"dj-4R4sbTdIq","execution":{"iopub.status.busy":"2024-01-06T10:54:29.181007Z","iopub.execute_input":"2024-01-06T10:54:29.181798Z","iopub.status.idle":"2024-01-06T10:54:29.188089Z","shell.execute_reply.started":"2024-01-06T10:54:29.181759Z","shell.execute_reply":"2024-01-06T10:54:29.187113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## For this session, we will just use first 2000 images and masks as our dataset"],"metadata":{"id":"4PjqMlM-TdIr"}},{"cell_type":"code","source":["# Use slicing concept to get only top 2000 images\n","images =\n","masks ="],"metadata":{"id":"oZrnkN5XTdIr","execution":{"iopub.status.busy":"2024-01-06T10:56:27.443102Z","iopub.execute_input":"2024-01-06T10:56:27.443721Z","iopub.status.idle":"2024-01-06T10:56:27.449228Z","shell.execute_reply.started":"2024-01-06T10:56:27.443657Z","shell.execute_reply":"2024-01-06T10:56:27.448241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the first image in images list --> Visualize it\n"],"metadata":{"id":"6EXl8jnlTdIs","execution":{"iopub.status.busy":"2024-01-06T10:59:07.772344Z","iopub.execute_input":"2024-01-06T10:59:07.772720Z","iopub.status.idle":"2024-01-06T10:59:07.949720Z","shell.execute_reply.started":"2024-01-06T10:59:07.772691Z","shell.execute_reply":"2024-01-06T10:59:07.948802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the respective mask --> Visualize it\n"],"metadata":{"id":"DaZRbOJFTdIs","execution":{"iopub.status.busy":"2024-01-06T10:59:10.822800Z","iopub.execute_input":"2024-01-06T10:59:10.823150Z","iopub.status.idle":"2024-01-06T10:59:10.861875Z","shell.execute_reply.started":"2024-01-06T10:59:10.823123Z","shell.execute_reply":"2024-01-06T10:59:10.861083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the shape of the image\n"],"metadata":{"id":"6FyHq7sYTdIs","execution":{"iopub.status.busy":"2024-01-06T10:59:13.031869Z","iopub.execute_input":"2024-01-06T10:59:13.032227Z","iopub.status.idle":"2024-01-06T10:59:13.038122Z","shell.execute_reply.started":"2024-01-06T10:59:13.032199Z","shell.execute_reply":"2024-01-06T10:59:13.037224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Originally, our images size is (720, 480) but we will reduce the size for better and faster processing. Since we are focusing on the clean masks so it will not effect much.\n","\n","### Ground masks are more detailed and have so much noise. We'll keep things easy for our lecture. However, feel free to use ground masks and play around to explore more."],"metadata":{"id":"akC9kBTmTdIt"}},{"cell_type":"code","source":["# Create H and W constants to save the height and the width of the image to pass to model and set it to 256\n","\n","\n","\n","# Empty list to store preprocessed images and masks\n","X_img = []\n","y_mask = []\n","\n","# Loop through each image and mask and implement the preprocessing steps\n","'''\n","Preprocessing Steps:-\n","\n","For image:-\n","1. Resize the image to 256 x 256\n","2. Normalize the image\n","3. Keep the data type as float (values between 0 to 1)\n","For mask:0\n","1. Resize the mask\n","2. Keep the data type as integer (Values between 0 - 255)\n","'''\n","for x, y in (zip(images, masks)):\n","    # preprocess image\n","\n","\n","    # preprocess mask\n","\n","\n","    # append the image and mask to respective list\n","\n"],"metadata":{"id":"-r412gybTdIt","execution":{"iopub.status.busy":"2024-01-06T11:08:56.533273Z","iopub.execute_input":"2024-01-06T11:08:56.533644Z","iopub.status.idle":"2024-01-06T11:09:55.352711Z","shell.execute_reply.started":"2024-01-06T11:08:56.533614Z","shell.execute_reply":"2024-01-06T11:09:55.351694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert X_img and y_mask lists to numpy array\n","\n","\n","# 1600 datapoints as training dataset and 400 for validation dataset using slicing\n","X_train =\n","X_valid =\n","\n","y_train =\n","y_valid =\n"],"metadata":{"id":"7GFo6HUcTdIu","execution":{"iopub.status.busy":"2024-01-06T11:11:28.072253Z","iopub.execute_input":"2024-01-06T11:11:28.072622Z","iopub.status.idle":"2024-01-06T11:11:28.703729Z","shell.execute_reply.started":"2024-01-06T11:11:28.072593Z","shell.execute_reply":"2024-01-06T11:11:28.702709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check shape of X_train\n"],"metadata":{"id":"qD4Nzq55TdIu","execution":{"iopub.status.busy":"2024-01-06T11:11:33.289735Z","iopub.execute_input":"2024-01-06T11:11:33.290077Z","iopub.status.idle":"2024-01-06T11:11:33.295914Z","shell.execute_reply.started":"2024-01-06T11:11:33.290049Z","shell.execute_reply":"2024-01-06T11:11:33.294962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a subplot to visualize image in X_train and respective y_train to see if everything is working fine\n"],"metadata":{"id":"8wxdt07iTdIv","execution":{"iopub.status.busy":"2024-01-06T11:11:49.087758Z","iopub.execute_input":"2024-01-06T11:11:49.088296Z","iopub.status.idle":"2024-01-06T11:11:49.643279Z","shell.execute_reply.started":"2024-01-06T11:11:49.088250Z","shell.execute_reply":"2024-01-06T11:11:49.642188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check this article to know more about how to build optimized data pipeline using tf\n","https://www.tensorflow.org/guide/data_performance"],"metadata":{"id":"5WP9p5NATdIv"}},{"cell_type":"markdown","source":["# Data Pipeline\n","\n","### One hot encoding\n","\n","![](https://i.imgur.com/mtimFxh.png)\n","\n","#### Similarly, we'll one hot encode our labels to 4 different channels for four classes"],"metadata":{"id":"ANA5KGoRTdIx"}},{"cell_type":"code","source":["batch_size =\n","num_classes =\n","\n","'''Here the from_tensor_slices function is called to make dataset objects of our training and validation sets'''\n","# calling tf_dataset\n","train_dataset =\n","valid_dataset ="],"metadata":{"id":"GEFDAxgITdIx","execution":{"iopub.status.busy":"2024-01-06T11:21:29.135400Z","iopub.execute_input":"2024-01-06T11:21:29.136295Z","iopub.status.idle":"2024-01-06T11:21:34.768074Z","shell.execute_reply.started":"2024-01-06T11:21:29.136264Z","shell.execute_reply":"2024-01-06T11:21:34.767167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Read more about prefetching and AUTOTUNE here: https://www.tensorflow.org/guide/data_performance#optimize_performance\n","\n","## Naive Approach\n","![](https://www.tensorflow.org/guide/images/data_performance/naive.svg)\n","\n","\n","## After prefetching\n","\n","![](https://www.tensorflow.org/guide/images/data_performance/prefetched.svg)"],"metadata":{"id":"MA6xzMK3TdIx"}},{"cell_type":"code","source":["# Batch the data and prefetch it\n","train_dataset =\n","valid_dataset ="],"metadata":{"id":"KiLkS-hZTdIx","execution":{"iopub.status.busy":"2024-01-06T11:32:08.355712Z","iopub.execute_input":"2024-01-06T11:32:08.356495Z","iopub.status.idle":"2024-01-06T11:32:08.368156Z","shell.execute_reply.started":"2024-01-06T11:32:08.356450Z","shell.execute_reply":"2024-01-06T11:32:08.367349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = iter(valid_dataset)\n","data = next(sample)\n","data[0].shape\n","# batch size, height, width, channels"],"metadata":{"id":"lWIoKssMTdIx","execution":{"iopub.status.busy":"2024-01-06T11:32:13.619659Z","iopub.execute_input":"2024-01-06T11:32:13.620360Z","iopub.status.idle":"2024-01-06T11:32:14.038784Z","shell.execute_reply.started":"2024-01-06T11:32:13.620330Z","shell.execute_reply":"2024-01-06T11:32:14.037925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[1].shape\n","# batch size, height, width, channels/classes"],"metadata":{"id":"SNNLqwmITdIy","execution":{"iopub.status.busy":"2024-01-06T09:24:31.929964Z","iopub.execute_input":"2024-01-06T09:24:31.930234Z","iopub.status.idle":"2024-01-06T09:24:31.935950Z","shell.execute_reply.started":"2024-01-06T09:24:31.930211Z","shell.execute_reply":"2024-01-06T09:24:31.934944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating U-net Architecture"],"metadata":{"id":"1MqxtDTmQXga"}},{"cell_type":"markdown","source":["**For Contracting Path:** the **conv_block** function is called four time which will create four block with pooling (pool = True). The process is repeated 3 more times.\n","\n","**For Bridge:** the **conv_block** function is called one time without pooling (pool=False).\n","\n","**For Expansive Path: UpSampling2D** is used to expands the size of images. This expanded  image is concatenated with the corresponding image from the contracting path, The reason here is to combine the information from the previous layers in order to get a more precise prediction. And now **conv_block** function is called without pooling (pool=False). The process is repeated 3 more times.\n","\n","The last step is to reshape the image to satisfy our prediction requirements. The last layer is a convolution layer with 1 filter of size 1x1."],"metadata":{"id":"8wNXCjt4QXgb"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate\n","from tensorflow.keras.models import Model\n","\n","'''conv_block it is used to create one block with two convolution layer\n","followed by BatchNormalization and activation function relu.\n","If the pooling is required then Maxpool2D is applied and return it else not.'''\n","# function to create convolution block\n","def conv_block(inputs, filters, pool=True):\n","  pass\n","\n","'''build_unet it is used to create the U-net architecture.'''\n","# function to build U-net\n","def build_unet(shape, num_classes):\n","    \"\"\" Input \"\"\"\n","\n","    \"\"\" Encoder \"\"\"\n","\n","\n","    \"\"\" Bridge \"\"\"\n","\n","    \"\"\" Decoder \"\"\"\n","    # Reference for UpSampling2D: https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\n","\n","    \"\"\" Output layer \"\"\"\n","\n","    pass"],"metadata":{"id":"tYCyf8smQXga","execution":{"iopub.status.busy":"2024-01-06T11:49:45.185289Z","iopub.execute_input":"2024-01-06T11:49:45.186160Z","iopub.status.idle":"2024-01-06T11:49:45.201610Z","shell.execute_reply.started":"2024-01-06T11:49:45.186125Z","shell.execute_reply":"2024-01-06T11:49:45.200621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calling build_unet function\n","model = build_unet()\n","\n","# Get the model summary\n"],"metadata":{"id":"65hPnreJQXgb","execution":{"iopub.status.busy":"2024-01-06T09:24:31.978904Z","iopub.execute_input":"2024-01-06T09:24:31.979262Z","iopub.status.idle":"2024-01-06T09:24:32.620642Z","shell.execute_reply.started":"2024-01-06T09:24:31.979226Z","shell.execute_reply":"2024-01-06T09:24:32.619781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load model and compile"],"metadata":{"id":"bMgeqmX2QXgc"}},{"cell_type":"code","source":["# install segmentation_models to get iou_score from metrics\n","!pip install segmentation_models"],"metadata":{"id":"jDWrKhNVlz7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import keras\n","\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\""],"metadata":{"id":"gRytQr2amEzz","executionInfo":{"status":"ok","timestamp":1715162125813,"user_tz":-330,"elapsed":773,"user":{"displayName":"Instructor at Spartificial","userId":"08630212477639634440"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import segmentation_models as sm\n","from segmentation_models.metrics import iou_score\n","\n","sm.set_framework('tf.keras')\n","keras.backend.set_image_data_format('channels_last')"],"metadata":{"id":"yjYVFu3zoS1W","executionInfo":{"status":"ok","timestamp":1715162134163,"user_tz":-330,"elapsed":497,"user":{"displayName":"Instructor at Spartificial","userId":"08630212477639634440"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["\"\"\" Hyperparameters \"\"\"\n","lr = 1e-4\n","epochs = 5\n","\n","\"\"\"Model\"\"\"\n","model.compile(loss=\"categorical_crossentropy\",       # jacard loss (try it!), dice_loss\n","              optimizer=tf.keras.optimizers.Adam(lr),\n","              metrics=[iou_score])\n","\n","\n","train_steps = len(X_train)//batch_size\n","valid_steps = len(X_valid)//batch_size"],"metadata":{"id":"z91qV2ZwQXgc","execution":{"iopub.status.busy":"2024-01-06T09:24:32.621832Z","iopub.execute_input":"2024-01-06T09:24:32.622122Z","iopub.status.idle":"2024-01-06T09:24:32.637043Z","shell.execute_reply.started":"2024-01-06T09:24:32.622096Z","shell.execute_reply":"2024-01-06T09:24:32.636094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_steps)\n","print(valid_steps)"],"metadata":{"execution":{"iopub.status.busy":"2024-01-06T09:26:21.403591Z","iopub.execute_input":"2024-01-06T09:26:21.404266Z","iopub.status.idle":"2024-01-06T09:26:21.408730Z","shell.execute_reply.started":"2024-01-06T09:26:21.404229Z","shell.execute_reply":"2024-01-06T09:26:21.407830Z"},"trusted":true,"id":"ZorpYnS5y8fD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train model"],"metadata":{"id":"SBhRPBKPQXgc"}},{"cell_type":"code","source":["'''model.fit is used to train the model'''\n","model_history = model.fit(train_dataset,\n","        steps_per_epoch=train_steps,\n","        validation_data=valid_dataset,\n","        validation_steps=valid_steps,\n","        epochs=epochs\n","    )"],"metadata":{"id":"4lJgBNVwQXgd","execution":{"iopub.status.busy":"2024-01-06T09:24:32.638268Z","iopub.execute_input":"2024-01-06T09:24:32.638530Z","iopub.status.idle":"2024-01-06T09:26:16.335450Z","shell.execute_reply.started":"2024-01-06T09:24:32.638507Z","shell.execute_reply":"2024-01-06T09:26:16.334456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predict from model"],"metadata":{"id":"9d0U7XJZQXgd"}},{"cell_type":"code","source":["# function to predict result\n","def predict_image(img_path, mask_path, model):\n","    H = 256\n","    W = 256\n","    num_classes = 4\n","\n","    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","    img = cv2.resize(img, (W, H))\n","    img = img / 255.0\n","    img = img.astype(np.float32)\n","\n","    ## Read mask\n","    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","    mask = cv2.resize(mask, (W, H))   ## (256, 256)\n","    mask = np.expand_dims(mask, axis=-1) ## (256, 256, 1)\n","    mask = mask * (255/num_classes)\n","    mask = mask.astype(np.int32)\n","    mask = np.concatenate([mask, mask, mask], axis=2)\n","\n","    ## Prediction\n","    pred_mask = model.predict(np.expand_dims(img, axis=0))[0] # (1, 256, 256, 3)\n","    pred_mask = np.argmax(pred_mask, axis=-1) # Output of pred_mask will be PROBABILITIES\n","    pred_mask = np.expand_dims(pred_mask, axis=-1)\n","    pred_mask = pred_mask * (255/num_classes)\n","    pred_mask = pred_mask.astype(np.int32)\n","    pred_mask = np.concatenate([pred_mask, pred_mask, pred_mask], axis=2)\n","\n","    return img, mask, pred_mask"],"metadata":{"id":"ey2fgIyiQXge","execution":{"iopub.status.busy":"2024-01-06T09:26:16.336873Z","iopub.execute_input":"2024-01-06T09:26:16.337251Z","iopub.status.idle":"2024-01-06T09:26:16.348252Z","shell.execute_reply.started":"2024-01-06T09:26:16.337216Z","shell.execute_reply":"2024-01-06T09:26:16.346819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to display result\n","def display(display_list):\n","  plt.figure(figsize=(12, 10))\n","\n","  title = ['Input Image', 'True Mask', 'Predicted Mask', 'Mask On Image']\n","\n","  for i in range(len(display_list)):\n","    plt.subplot(1, len(display_list), i+1)\n","    plt.title(title[i])\n","    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n","    plt.axis('off')\n","  plt.show()"],"metadata":{"id":"y_f35xF5QXge","execution":{"iopub.status.busy":"2024-01-06T09:26:16.349705Z","iopub.execute_input":"2024-01-06T09:26:16.350085Z","iopub.status.idle":"2024-01-06T09:26:16.360233Z","shell.execute_reply.started":"2024-01-06T09:26:16.350052Z","shell.execute_reply":"2024-01-06T09:26:16.359342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_path = '../input/artificial-lunar-rocky-landscape-dataset/images/render/render0041.png'\n","mask_path = '../input/artificial-lunar-rocky-landscape-dataset/images/clean/clean0041.png'\n","\n","img, mask, pred_mask = predict_image(img_path, mask_path, model)\n","\n","display([img, mask, pred_mask])"],"metadata":{"id":"rhP2yyG-QXge","execution":{"iopub.status.busy":"2024-01-06T09:26:16.361787Z","iopub.execute_input":"2024-01-06T09:26:16.362539Z","iopub.status.idle":"2024-01-06T09:26:17.128573Z","shell.execute_reply.started":"2024-01-06T09:26:16.362509Z","shell.execute_reply":"2024-01-06T09:26:17.127700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## A practical note: different backbones in modern U-Nets\n","\n","So far, you have looked at how the U-Net architecture was implemented in the original work by Ronneberger et al. Over the years, many people have experienced with different setups for U-Nets, including pretraining on e.g. ImageNet and then finetuning to their specific image segmentation tasks.\n","\n","This means that today, you will likely use a U-Net that no longer utilizes the original architecture as proposed above - but it's still a good starting point, because the contractive path, expansive path and the skip connections remain the same.\n","\n","**Common backbones for U-Net architectures these days are ResNet, ResNeXt, EfficientNet and DenseNet architectures. Often, these have been pretrained on the ImageNet dataset, so that many common features have already been learned. By using these backbone U-Nets, initialized with pretrained weights, it's likely that you can reach convergence on your segmentation problem much faster.**\n","\n","That's it! You have now a high-level understanding of U-Net and its components sunglasses.\n","\n","## In the next module, we will learn how you can use segmentation_models using Transfer learning to use UNet architecture with different pretrained models as backbone."],"metadata":{"id":"9On3t8XsTdI2"}}]}